# American Dream

The "American Dream" has always been a funny concept.

I am an immigrant, having moved to Southern California when I was only 2 years old.

I don't have any memories from where I was born---Baghdad, Iraq. I don't have any memories
from where my family moved to when I was only 10 months old---Amman, Jordan. I only know
America.

I never quite understood what the "American Dream" meant to white Americans who were born
in the USA and came from several generations of American citizens.

In regards to my parents, they already achieved the "American Dream:" we moved to America.

That's it. Moving to America.

The irony doesn't escape me. They achieved the "American Dream" by moving to America when
the USA's invasion of Iraq only added to the hostility that drove them out.

Funnily enough, I never saw America as a bad country as I was growing up. I only thought
that certain bad situations involved America or took place in America. I never really
connected the dots between those tragedies and the foundations and structure of the USA,
though.

Until about 7th grade, I thought there were simply a few bad apples in government and
public service roles, simply a few racist and homophobic people, simply a few instances of
injustice, etc., etc.---you get the point.

As I grew up and became more and more exposed to the world beyond my little bubble, I came
to realize that I was wrong. I was so very wrong.

Our government is built on the backs of enslaved African-Americans and of immigrants, who
were targeted and exploited so that white settlers could pursue their "American Dream."
(I find the argument that immigrants stole "American" jobs so dumb. Can you tell?)
Americans even exploited Irish Catholics who immigrated in hopes of escaping famine, death,
and poverty, only to find themselves as targets of Protestant Irish and British settlers.

???

White Americans gatekeep a country they stole and to which they have no rights. Oh the irony.

Stealing land from Indigenous peoples and murdering them to pursue "Manifest
Destiny" and achieve the "American Dream" is just so fundamentally wrong and cruel, and the
fact that some Americans don't see it as such is because America doesn't want to admit it was
wrong.

Growing up, I was surrounded by conservative family, family friends, and classmates in a Middle
Eastern diasporic community, which basically thinks the world of the USA because they
can only compare it to the tyranny and war zones of the Middle East in the 2000s and before that,
namely in Iraq (again, the irony is not lost on me). You may see that this was a teensy bit of a
problem when I say that I am liberal, but I was fortunate that my parents chose to settle in
Southern California is all places (albeit in a more conservative and inland region). The history
teachers that I remember (those from high school) were *also* liberal, so they gave us the
straight-up facts about the USA and did *not* sugarcoat anything. They openly bashed America when
it was deserving of such a reaction, which was relatively often in American history.

In so many other states, the government, curriculum, and teachers *do* sugarcoat the fact that
Europeans nearly wiped out Indigenous peoples when they decided to mosey on over and expose them all
to diseases to which they had not built immunity, the fact that the Trail of Tears and Japanese
internment campes were two of the most inhumane acts of terror ever orchestrated on American soil,
the fact that enslaved Black people have been experiencing cycles of generational trauma ever since
white people decided that Black people were "not equal" because their skin is darker (when it is
literally just an adaptation to the sun, mind you), the fact that systemic racism is a thing that
*exists* that puts BIPOC at a disadvantage in every aspect of their lives, and so much more.

There is so much that the USA doesn't completely own up to, and white Americans don't accept that
America is **not** the greatest country in the world. This is *pure arrogance*, and the arrogance of
a country that refuses to acknowledge its flaws, cruelty, and injustice (past and present) is the reason
why the world laughs at America and why I will never believe in the "American Dream."

America simply cannot grow until it not only accepts and acknowledges its wrongdoings but also *makes
things right* as fast as possible and in every way possible. Until that happens, there really is no
"American Dream" because a country that cannot grow is a country that cannot fulfill a dream.

Regards, L.S.
